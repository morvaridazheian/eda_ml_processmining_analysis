# -*- coding: utf-8 -*-
"""ML-my Data report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uCAbfYWO5KATAaYAv1686CYqZ3fd6AKe

# 1) Import & Load Data
"""

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""## 1-1) Read Data"""

# Read Excel file
df = pd.read_excel("sample_process_mining_20rows.xlsx")

# Standardize column names
df.columns = df.columns.str.lower().str.strip()

"""## 2-2) Case Features Overview

Case duration (activity duration) = “On average, how much time passes between two consecutive activities in a case?” => Helps analyze delays between activities

Throughput time = “How long does the whole case take from start to finish?” => Includes all waiting, idle, and processing time
"""

# Ensure timestamps are datetime
df['start_timestamp'] = pd.to_datetime(df['start_timestamp'])
df['resolved_date'] = pd.to_datetime(df['resolved_date'])

# Calculate activity duration in HOURS
df['next_timestamp'] = df.groupby('case_id')['start_timestamp'].shift(-1)

df['activity_duration_hours'] = (
    (df['next_timestamp'] - df['start_timestamp']).dt.total_seconds() / 3600
)

df['activity_duration_months'] = (
    (df['next_timestamp'] - df['start_timestamp']).dt.total_seconds() / (3600*24*30)
)



# Aggregate per case
case_features_summary = df.groupby('case_id').agg(
    num_activities=('activity', 'count'),
    num_unique_activities=('activity', 'nunique'),
    avg_activity_duration_Hours=('activity_duration_hours','mean'),
    avg_activity_duration_Months=('activity_duration_months','mean'),
    total_effort=('effort_hours','sum'),
    total_estimated=('estimated_hours','sum'),
    first_activity=('activity','first'),
    last_activity=('activity','last'),
    first_start=('start_timestamp','min'),
    last_end=('start_timestamp','max'),
    most_common_resource=('resource', lambda x: x.value_counts().idxmax()),
    most_common_originator=('originator', lambda x: x.value_counts().idxmax()),
    most_common_issue_type=('issue_type', lambda x: x.value_counts().idxmax())
).reset_index()




# calculate throughput_hours & months per case
case_features_summary['throughput_hours'] = (case_features_summary['last_end'] - case_features_summary['first_start']).dt.total_seconds() / 3600
case_features_summary['throughput_months'] = (case_features_summary['last_end'] - case_features_summary['first_start']).dt.total_seconds() / (3600 * 24 * 30)


case_features_summary= case_features_summary[['case_id','num_activities', 'num_unique_activities','avg_activity_duration_Hours','avg_activity_duration_Months',
                               'throughput_hours','throughput_months','total_effort', 'total_estimated',
                                'first_activity', 'last_activity', 'first_start', 'last_end',
                                'most_common_resource', 'most_common_originator','most_common_issue_type']]
# Show first rows
case_features_summary.head().round(2)

"""Below we see almost all cases with less than 8 hours TPT, except one, have the last_activity="CANCELED"
"""

case_features_summary[case_features_summary['throughput_hours'] <= 8 ].round(2)

"""# 2) Predictive Analytics

Aggregate features per case
"""

# Aggregate features per case
case_features = df.groupby('case_id').agg(
    num_activities=('activity','count'),
    avg_activity_duration=('activity_duration_hours','mean'),
    total_effort=('effort_hours','sum'),
    total_estimated=('estimated_hours','sum'),
    first_activity=('activity','first'),
    issue_type=('issue_type','first'),
    originator=('originator','first'),
    resource=('resource','first'),
    case_start=('start_timestamp','min'),
    case_end=('start_timestamp','max')
).reset_index()

"""## 2-1) Predict Throughput Time (Regression)"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# Target: throughput_hours
case_features['throughput_hours'] = (case_features['case_end'] - case_features['case_start']).dt.total_seconds()/3600

# Features for ML
X = case_features[['issue_type','originator','resource','num_activities','avg_activity_duration','total_effort','total_estimated']]
y = case_features['throughput_hours']

# One-hot encode categorical variables
X = pd.get_dummies(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model: Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)


# Metrics
#rmse = mean_squared_error(y_test, y_pred, squared=False)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R² score: {r2:.2f}")
print("\n")
print(f"On average the predictions are off by {rmse:.2f} unit")
print("The ration of RMSE to mean of target is:", round(rmse/y.mean(),2),"\n")
print("How the model is performing? ")
print("If the ration < 10% => very Good!")
print("If the ration 20-30% => Moderate!")
print("If the ration > 50% => Poor!")

# Comparison table: predicted vs actual
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Error': y_test - y_pred
}).reset_index(drop=True)
comparison.round(2)  # show first 10 rows

# Scatter plot: Predicted vs Actual
plt.figure(figsize=(4,3))
plt.scatter(y_test, y_pred, alpha=0.6, color='teal', edgecolor='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # perfect fit line
plt.xlabel('Actual Throughput Hours')
plt.ylabel('Predicted Throughput Hours')
plt.title('Random Forest Regression: Predicted vs Actual')
plt.grid(True)
plt.show()

"""## 2-2) Predict Likelihood of Rework (Classification)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Create target: rework flag (1 if case has repeated activity)
rework_flag = df.groupby('case_id')['activity'].apply(lambda x: x.duplicated().any()).astype(int)
case_features['rework_flag'] = case_features['case_id'].map(rework_flag)

# Features
X = case_features[['first_activity','originator','resource','issue_type','num_activities','avg_activity_duration','total_effort','total_estimated']]
y = case_features['rework_flag']

# One-hot encode categorical features
X = pd.get_dummies(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model: Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)



# Metrics
accuracy = accuracy_score(y_test, y_pred)

print("The ration of 'correct predictions' to 'total predictions' is:" )
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("If High precision + High recall (High f1-score) => Model performs Great!")
# Comparison table: predicted vs actual
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred
}).reset_index(drop=True)
print("\nComparison Table (first 10 rows):")
comparison

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
plt.title('Confusion Matrix')
plt.show()

"""## 2-3) Predict Case Issue Type (Classification)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Filter dataset: only keep rows with non-NaN issue_type
df_clean = df[df['issue_type'].notna()].copy()

# Create case-level features
case_features_clean = df_clean.groupby('case_id').agg(
    first_activity=('activity', 'first'),
    originator=('originator', 'first'),
    resource=('resource', 'first'),
    num_activities=('activity', 'count'),
    avg_activity_duration=('activity_duration_hours', 'mean'),
    total_effort=('effort_hours', 'sum'),
    total_estimated=('estimated_hours', 'sum')
).reset_index()


# Target: final issue_type of the case
case_features_clean['final_issue_type'] = df_clean.groupby('case_id')['issue_type'].last().values

# Features and target
X = case_features_clean[['first_activity','originator','resource','num_activities','avg_activity_duration','total_effort','total_estimated']]
y = case_features_clean['final_issue_type']

# One-hot encode categorical variables
X = pd.get_dummies(X)

# Train-test split
#----Stratified splitting requires at least 2 samples per class so that both train and test sets contain examples of every class.
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
#----For smaller samples remove stratify=y.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model: Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Metrics
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("If f1 score for one type is higher than the other => The dataset is imbalaced!" )

# Comparison table: predicted vs actual
comparison = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred
}).reset_index(drop=True)
print("\nComparison Table :")
comparison

# Confusion Matrix

cm = confusion_matrix(y_test, y_pred, labels=rf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)
disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
plt.title('Confusion Matrix')
plt.show()

"""# 3) Process Mining + Clustering

## 3-1) Case Clustering by Process Behavior

Showing the number of cases that had similar process behavior in each cluster.
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# --- Prepare case-level features ---
case_sequences = df.groupby('case_id')['activity'].apply(list)

# One-hot encode activity sequences (presence counts)
unique_activities = df['activity'].unique()
encoded_sequences = pd.DataFrame(0, index=case_sequences.index, columns=unique_activities)

for case_id, activities in case_sequences.items():
    for act in activities:
        encoded_sequences.loc[case_id, act] += 1  # count occurrence

# Add numeric features
encoded_sequences['num_activities'] = case_sequences.apply(len)
encoded_sequences['avg_activity_duration'] = df.groupby('case_id')['activity_duration_hours'].mean()

# --- KMeans Clustering ---
kmeans = KMeans(n_clusters=5, random_state=42)
encoded_sequences['cluster'] = kmeans.fit_predict(encoded_sequences)

print("Cluster counts show the number of cases that had similar process behaviour:")
activity_cluster_count = encoded_sequences['cluster'].value_counts()
print(activity_cluster_count)

# --- Unsupervised Evaluation Metrics ---
#Inertia (within-cluster sum-of-squares) – lower is better
inertia = kmeans.inertia_

#Silhouette Score – measures how well-separated clusters are (range: -1 to 1, higher is better)
sil_score = silhouette_score(encoded_sequences.drop('cluster', axis=1), encoded_sequences['cluster'])

#Calinski-Harabasz Index – higher values indicate better-defined clusters
ch_score = calinski_harabasz_score(encoded_sequences.drop('cluster', axis=1), encoded_sequences['cluster'])

#print(f"Inertia (within-cluster sum-of-squares): {inertia:.2f}")
print("How well-separated clusters are (range: -1 to 1, higher is better)!")
print(f"Silhouette Score: {sil_score:.4f}")
#print(f"Calinski-Harabasz Index: {ch_score:.2f}")

# --- Visualizations ---
# Cluster counts bar plot
plt.figure(figsize=(4,3))
encoded_sequences['cluster'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('Cluster')
plt.ylabel('Number of Cases')
plt.title('Number of Cases per Cluster')
plt.show()

"""Showing Process variants per cluster:"""

# Create process variant per case
case_variants = df.groupby('case_id')['activity'].apply(
    lambda x: ' → '.join(x)
)
case_cluster_variants = pd.DataFrame({
    'case_id': case_variants.index,
    'process_variant': case_variants.values,
    'cluster': encoded_sequences['cluster'].values
}) # each row = one case with its full process and cluster.

variant_frequency_per_cluster = (
    case_cluster_variants
    .groupby(['cluster', 'process_variant'])
    .size()
    .reset_index(name='case_count')
    .sort_values(['cluster', 'case_count'], ascending=[True, False])
)

#top N processes per cluster
top_variants_per_cluster = (
    variant_frequency_per_cluster
    .groupby('cluster')
    .head(5)
)

top_variants_per_cluster

"""## 3-2) Resource Clustering by Workload

Resources with similar features in the same cluster.
"""

# Aggregate workload per resource
resource_workload = df.groupby('resource').agg(
    total_effort=('effort_hours','sum'),
    avg_activity_duration_hours=('activity_duration_hours','mean'),
    avg_activity_duration_months=('activity_duration_months','mean'),
    num_cases=('case_id','nunique'),
    rework_count=('activity', lambda x: x.duplicated().sum())
)

# KMeans clustering
kmeans_res = KMeans(n_clusters=3, random_state=42)
resource_workload['cluster'] = kmeans_res.fit_predict(resource_workload)

resource_workload.round(2)

# ---- Unsupervised Evaluation Metrics ----
silhouette = silhouette_score(resource_workload.drop('cluster', axis=1), resource_workload['cluster'])

print("\n--- Clustering Evaluation Metrics ---")
print(f"Silhouette Score (−1 to 1): {silhouette:.4f}")


# ---- Visualizations ----

# 1. Bar chart for cluster distribution
plt.figure(figsize=(4,3))
resource_workload['cluster'].value_counts().sort_index().plot(
    kind='bar', color='skyblue', edgecolor='black'
)
plt.title("Number of Resources per Cluster")
plt.xlabel("Cluster")
plt.ylabel("Count of Resources")
plt.show()

"""# 4) Anomaly Detection (Potential Bottlenecks)

Prediction output: 1 → normal case; -1 → anomaly

From the output we can: 1) Identify potential bottlenecks: Cases with unusually long throughput_hours or avg_activity_duration. 2) Spot rework-heavy cases: Cases with a very high number of activities or repeated efforts. 3) Prioritize process improvement: Investigate why these anomalous cases exist (process inefficiency, unclear responsibilities, system delays).
"""

from sklearn.svm import OneClassSVM

# Features per case
case_ml_features = df.groupby('case_id').agg(
    first_start=('start_timestamp', 'min'),
    last_start=('start_timestamp', 'max'),
    num_activities=('activity','count'),
    total_effort=('effort_hours','sum'),
    avg_activity_duration_hours=('activity_duration_hours','mean'),
    avg_activity_duration_months=('activity_duration_months','mean')
).fillna(0)

# Calculate throughput time in hours
case_ml_features['throughput_hours'] = (
    (case_ml_features['last_start'] - case_ml_features['first_start']).dt.total_seconds() / 3600
)
case_ml_features['throughput_months'] = (
    (case_ml_features['last_start'] - case_ml_features['first_start']).dt.total_seconds() / (3600*24*30)
)
# Drop helper columns
case_ml_features = case_ml_features.drop(columns=['first_start', 'last_start'])

# Feature matrix
X = case_ml_features[['throughput_hours','num_activities','total_effort','avg_activity_duration_hours']]

# One-Class SVM
ocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')
ocsvm.fit(X)
case_ml_features['anomaly'] = ocsvm.predict(X)

print("Now! you can identify cases that are to be anomaly:")
# -1 indicates anomaly, 1 indicates normal
case_ml_features[case_ml_features['anomaly'] == -1].round(2)

plt.scatter(case_ml_features['num_activities'], case_ml_features['avg_activity_duration_hours'],
            c=case_ml_features['anomaly'].map({1:'blue', -1:'red'}))
plt.xlabel('Number of Activities')
plt.ylabel('Average Activity Duration (hours)')
plt.title('Anomaly Detection: Cases')
plt.show()

"""# 5) Sequence Modeling

Useful for real-time process monitoring, Bottleneck detection, Resource allocation, etc.

Next activity prediction: For any ongoing case, given the activities so far, predict the next likely activity.

## 5-1) Predict Mode: "Next Activity"
"""

#from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding


# Prepare sequences
case_sequences = df.groupby('case_id')['activity'].apply(list).tolist()

# Flatten activities and tokenize
tokenizer = Tokenizer()
tokenizer.fit_on_texts(case_sequences)
sequences = tokenizer.texts_to_sequences(case_sequences)

# Pad sequences
max_len = max(len(seq) for seq in sequences)
X_seq = pad_sequences(sequences, maxlen=max_len, padding='post')

# Target: next activity (shifted sequences)
y_seq = np.roll(X_seq, -1, axis=1)

# Simple LSTM model for next activity prediction
vocab_size = len(tokenizer.word_index) + 1
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_len))
model.add(LSTM(64, return_sequences=True))
model.add(Dense(vocab_size, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
model.fit(X_seq, y_seq, epochs=10, batch_size=32)  # Uncomment to train

"""### Example:"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Example: current activities of a new or ongoing case
new_case = ['Approve', 'Review', 'Implement']

# Convert activity names to integer tokens
seq_tokens = tokenizer.texts_to_sequences([new_case])
seq_padded = pad_sequences(seq_tokens, maxlen=max_len, padding='post')

# Predict probabilities for next activity
pred_probs = model.predict(seq_padded, verbose=0)
next_step_probs = pred_probs[0, len(new_case)-1]  # probabilities for next activity

# -----------------------------
# Get predicted next activity
# Exclude padding index (0)
next_step_probs[0] = -np.inf  # ensure padding is never predicted
next_activity_token = np.argmax(next_step_probs)
next_activity_name = tokenizer.index_word[next_activity_token]

print("Predicted next activity:", next_activity_name)

# -----------------------------
# Top-K predictions
top_k = 3
valid_indices = np.argsort(next_step_probs)[::-1]  # descending order
valid_indices = [i for i in valid_indices if i != 0]  # remove padding
top_indices = valid_indices[:top_k]
top_activities = [tokenizer.index_word[i] for i in top_indices]

print(f"Top {top_k} predicted next activities:", top_activities)



"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4df60853-7a15-4a5a-90b9-fad5d9cfd31d' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""
# -*- coding: utf-8 -*-
"""EDA-my Data report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GPm0B0dUdeZcgX6bG9Djfexg2V7rFEk7

# 1) Import & Load Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx

df = pd.read_excel("sample_process_mining_20rows.xlsx", engine='openpyxl')

df.columns = df.columns.str.lower().str.strip()

df.info()

df.describe().round(2)

"""## 1-1) Convert Timestamps

Here our default data time for time columns are "datetime" so we can skip converting them
"""

df['start_timestamp'] = pd.to_datetime(df['start_timestamp'])
df['resolved_date'] = pd.to_datetime(df['resolved_date'], errors='coerce')

df['date'] = df['start_timestamp'].dt.date
df['hour'] = df['start_timestamp'].dt.hour

daily_hours = df.groupby('date')['hour'].agg(min_hour='min', max_hour='max')

daily_hours

"""# 2) Basic Data Quality Checks

## 2-1) Missing values
"""

df.isna().sum()

"""## 2-2) Duplicates

checking no fully identical "rows"
"""

df.duplicated().sum()

"""checking duplicates across multiple columns"""

dup_uni = pd.DataFrame({
    "unique_count": df.nunique(),
    "duplicate_count": df.apply(lambda col: col.duplicated().sum())
})

dup_uni

"""# 3) Compute Case Duration (Throughput Time)

Case End = Last event timestamp (latest start_timestamp) => case duration = ('start_timestamp','max')-('start_timestamp','min')
"""

case_times = df.groupby('case_id').agg(
    case_start=('start_timestamp','min'),
    case_end=('start_timestamp','max')
)

case_times['case_duration_hours'] = (
    (case_times['case_end'] - case_times['case_start'])
    .dt.total_seconds() / 3600
).round(2)

# Given that 1 month = 30 days = 720 hours
case_times['case_duration_months'] = (
    (case_times['case_end'] - case_times['case_start']).dt.total_seconds()
    / (3600 * 24 * 30)
).round(2)

case_times.head(10)

"""# 4) EDA Questions + Code + Visuals

## 4-A) Number of Cases & Activities:
"""

dup_uni.loc[['case_id','activity']]

"""## 4-B) Event distribution :"""

df['activity'].value_counts()

df['activity'].value_counts().plot(kind='bar', figsize=(8,4))

"""## 4-C) Events per case (case size):"""

df['case_id'].value_counts()

df['case_id'].value_counts().plot(
    kind='hist',         # keep it as histogram
    bins=15,             # number of bins (adjust for detail)
    color='skyblue',     # bar color
    edgecolor='black',   # border for bars
    alpha=0.7,           # transparency
    figsize=(10,6),      # figure size
    grid=True,           # show grid
    title='Distribution of Number of Events per Case'  # plot title
)

plt.xlabel('Number of Events per Case')
plt.ylabel('Number of Cases')
plt.show()

"""## 4-D) Waiting time between consecutive events:

computing "waiting time" between events inside each case
"""

df['next_timestamp'] = df.groupby('case_id')['start_timestamp'].shift(-1)

df['waiting_time_hours'] = (
    (df['next_timestamp'] - df['start_timestamp'])
         .dt.total_seconds() / 3600
).round(2)

# Given that 1 month = 30 days = 720 hours
df['waiting_time_months'] = (
    (df['next_timestamp'] - df['start_timestamp'])
         .dt.total_seconds() / (3600 * 24 * 30)
).round(2)

df[['waiting_time_hours','waiting_time_months']].describe().round(2)

"""## 4-E) Average waiting time per activity

This identifies bottlenecks.
"""

df.groupby('activity')[['waiting_time_hours','waiting_time_months']].mean().sort_values(by='waiting_time_hours', ascending=False).round(2)

(
    df.groupby('activity')
      .agg(
          waiting_time_hours=('waiting_time_hours', 'mean'),
          waiting_time_months=('waiting_time_months', 'mean'),
          frequency=('activity', 'count')
      )
      .sort_values(by='waiting_time_hours', ascending=False)
      .round(2)
)

"""## 4-F) Workload Analysis - resource & originator

### Resource:
"""

# 1) Number of events per 'resource'
events_by_resource = df['resource'].value_counts()

# 2) Number of unique cases each resource performs
unique_cases_by_resource = df.groupby('resource')['case_id'].nunique()

# 3) Number of unique activities each resource performs
unique_activities_by_resource = df.groupby('resource')['activity'].nunique()

# --- Print summary table ---
resource_summary = pd.DataFrame({
    'event_count': events_by_resource,
    'unique_cases': unique_cases_by_resource,
    'unique_activities': unique_activities_by_resource
}).sort_values('event_count', ascending=False)

resource_summary

"""Activity Distribution by Resource:

We consider event sequence inside each case.
"""

pivot_activity = df.pivot_table(
    index='resource',
    columns='activity',
    values='case_id',
    aggfunc='count',
    fill_value=0
)

plt.figure(figsize=(14,6))
sns.heatmap(pivot_activity, cmap="Blues")
plt.title("Activity Distribution by Resource")
plt.xlabel("Activity")
plt.ylabel("Resource")
plt.tight_layout()
plt.show()

df['next_resource'] = df.groupby('case_id')['resource'].shift(-1)
handover_resource = (
    df.groupby(['resource', 'next_resource'])
      .size()
      .reset_index(name='count')
      .dropna()
)

handover_resource.head()

"""Handover of work among resources:"""

Gr = nx.from_pandas_edgelist(
    handover_resource,
    source='resource',
    target='next_resource',
    edge_attr='count',
    create_using=nx.DiGraph()
)

plt.figure(figsize=(12,8))
pos = nx.spring_layout(Gr, k=0.4)
nx.draw(Gr, pos, with_labels=True, node_size=2500, node_color='lightgray', arrowsize=20)
plt.title("Handover of Work Network (Resources)")
plt.show()

"""=> This network shows that each resource does a task independently from the others

### Originator:
"""

# 1) Number of events per originator
events_by_originator = df['originator'].value_counts()

# 2) Number of unique cases each originator performs
unique_cases_by_originator = df.groupby('originator')['case_id'].nunique()

# 3) Number of unique activities each originator performs
unique_activities_by_originator = df.groupby('originator')['activity'].nunique()

# --- Print summary table ---
originator_summary = pd.DataFrame({
    'event_count': events_by_originator,
    'unique_cases': unique_cases_by_originator,
    'unique_activities': unique_activities_by_originator
}).sort_values('event_count', ascending=False)

originator_summary

"""Activity Distribution by Originator"""

pivot_activity = df.pivot_table(
    index='originator',
    columns='activity',
    values='case_id',
    aggfunc='count',
    fill_value=0
)

plt.figure(figsize=(14,6))
sns.heatmap(pivot_activity, cmap="Blues")
plt.title("Activity Distribution by Originator")
plt.xlabel("Activity")
plt.ylabel("Originator")
plt.tight_layout()
plt.show()

"""Handover of work among originators:"""

df['next_originator'] = df.groupby('case_id')['originator'].shift(-1)
handover = (
    df.groupby(['originator', 'next_originator'])
      .size()
      .reset_index(name='count')
      .dropna()
)

handover.head()

G = nx.from_pandas_edgelist(
    handover,
    source='originator',
    target='next_originator',
    edge_attr='count',
    create_using=nx.DiGraph()
)

plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.4)
nx.draw(G, pos, with_labels=True, node_size=2500, node_color='lightgray', arrowsize=20)
plt.title("Handover of Work Network")
plt.show()

"""=> This network shows that each originator asks for a task independently from the others

## 4-G) Issue-type Analysis

Effort-hours per "issue type" summary:
"""

# Count of events per issue type
issue_count = df['issue_type'].value_counts()

# Total effort hours per issue type
total_effort = df.groupby('issue_type')['effort_hours'].sum()

# Average effort hours per issue type
avg_effort = df.groupby('issue_type')['effort_hours'].mean()

# Number of unique cases/events per issue type
unique_cases = df.groupby('issue_type')['case_id'].nunique()

# Combine into a single DataFrame
issue_summary = pd.DataFrame({
    'count': issue_count,
    'unique_cases': unique_cases,
    'total_effort_hours': total_effort,
    'avg_effort_hours': avg_effort
})

# Optional: sort by count descending
issue_summary = issue_summary.sort_values(by='count', ascending=False)

issue_summary.round(2)

"""## 4-H) Sequence (Variant) Analysis"""

variants = df.groupby('case_id')['activity'].apply(list)
variants.value_counts().head(15)

# 1. Original variants (must be used exactly)
variants = df.groupby('case_id')['activity'].apply(list)

# 2. Throughput time per case
case_times_variant = df.groupby('case_id').agg(
    case_start=('start_timestamp', 'min'),
    case_end=('start_timestamp', 'max')
)

case_times_variant['hours'] = (
    (case_times_variant['case_end'] - case_times_variant['case_start'])
    .dt.total_seconds() / 3600
)

case_times_variant['months'] = (
    (case_times_variant['case_end'] - case_times_variant['case_start'])
    .dt.total_seconds() / (3600 * 24 * 30)
)

# 3. Create variant_df aligned with variants
variant_df = pd.DataFrame({
    'variant': variants.apply(tuple),
    'hours': case_times_variant['hours'].values,
    'months': case_times_variant['months'].values
})

total_cases = len(variant_df)

# 4. Group and compute summary stats
result = (
    variant_df.groupby('variant')
    .agg(
        frequency=('variant', 'size'),
        avg_hours=('hours', 'mean'),
        median_hours=('hours', 'median'),
        avg_months=('months', 'mean'),
        median_months=('months', 'median')
    )
    .sort_values(by='frequency', ascending=False)
    .reset_index()
)

# 5. Add % of total
result['percent_of_total'] = (result['frequency'] / total_cases * 100).round(2)

# Make variant readable lists
result['variant'] = result['variant'].apply(list)

result.head(15).round(2)

"""## 4-I) Case Duration Distribution"""

case_times['case_duration_days'] = (case_times['case_duration_hours'] / 24).round(2)

case_times.describe().round(2)

plt.figure(figsize=(10,6))

# Histogram
plt.hist(case_times['case_duration_months'], bins=30, edgecolor='black', alpha=0.7)

# Labels
plt.title("Distribution of Case Throughput Time (Months)", fontsize=14)
plt.xlabel("Case Duration (Months)", fontsize=12)
plt.ylabel("Number of Cases", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Add mean & median lines
mean_val = case_times['case_duration_months'].mean()
median_val = case_times['case_duration_months'].median()

plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f"Mean = {mean_val:.2f} months")
plt.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f"Median = {median_val:.2f} months")

plt.legend()
plt.show()

"""### case frequency per 2 years:"""

# Extract the start year
df['start_year'] = df['start_timestamp'].dt.year

# Compute 2-year period by flooring to the nearest even year
df['2yr_period'] = (df['start_year'] // 2) * 2

# Format as string "YYYY–YYYY+1"
df['2yr_period_str'] = df['2yr_period'].astype(str) + '–' + (df['2yr_period'] + 1).astype(str)

# Count unique cases per 2-year period
cases_per_2yr = df.groupby('2yr_period_str')['case_id'].nunique()

cases_per_2yr

# Plot time-series
plt.figure(figsize=(10,5))
cases_per_2yr.plot(marker='o', linestyle='-', color='skyblue', title='Number of Cases per 2-Year Period')
plt.xlabel('2-Year Period Start')
plt.ylabel('Number of Cases')
plt.grid(True)
plt.show()

"""## 4-J) Monthly case arrivals"""

df['month'] = df['start_timestamp'].dt.to_period('M')
df['month'].value_counts().sort_index()

# Extract month as Period
df['month'] = df['start_timestamp'].dt.to_period('M')

# Extract year
df['year'] = df['start_timestamp'].dt.year

# Prepare the figure
plt.figure(figsize=(12,6))

# Plot each year separately
for year, group in df.groupby('year'):
    monthly_counts = group['month'].value_counts().sort_index()
    monthly_counts.plot(
        kind='line',
        marker='o',
        linestyle='-',
        label=str(year)
    )

plt.title('Number of Cases per Month by Year')
plt.xlabel('Month')
plt.ylabel('Number of Cases')
plt.grid(True)
plt.legend(title='Year')
plt.xticks(rotation=45)
plt.show()

"""## 4-K) SLA Analysis

Using resolved_date vs first event (start_timestamp):
"""

case_resolution = df.groupby('case_id').agg(
    first_start=('start_timestamp','min'),
    resolved_date=('resolved_date','max')
)

case_resolution['sla_days'] = (
    case_resolution['resolved_date'] - case_resolution['first_start']
).dt.days

"""### Numerical Analysis - % of cases exceeding SLA threshold"""

# Descriptive statistics
sla_stats = case_resolution['sla_days'].describe().round(2)
print("SLA Descriptive Statistics:")
sla_stats

"""Distribution of SLA => Highlights long delays/outliers"""

plt.figure(figsize=(10,5))
plt.hist(case_resolution['sla_days'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
plt.title('Distribution of SLA (Days)')
plt.xlabel('SLA Days')
plt.ylabel('Number of Cases')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""cases meeting/exceeding SLA threshold

SLA = the time allowed to resolve a case

resolved_date - start_timestamp = actual duration, which we compare with SLA targets to check compliance

If the actual duration > SLA target → SLA breached, otherwise → SLA met
"""

# Count cases meeting/exceeding SLA threshold (e.g., 90 days)
sla_threshold = 90
sla_exceeded = (case_resolution['sla_days'] > sla_threshold).sum()
total_cases = len(case_resolution)
print(f"\nCases exceeding {sla_threshold} days SLA: {sla_exceeded} / {total_cases} ({(sla_exceeded/total_cases*100):.2f}%)")

"""SLA = 0 => instant or very quick resolution.

SLA = -1 => Data entry errors (wrong timestamps), Timezone differences, Timezone differences
"""

cases_sla_neg1_0 = case_resolution[case_resolution['sla_days'].isin([-1, 0])]
cases_sla_neg1_0


#case_resolution['sla_days'].isin([-1, 0]).sum()

"""### SLA over Time (Trend Analysis)

Shows trend of resolved cases over months

cases_resolved column=> number of unique cases resolved in each month.

avg_sla_days column =>  average SLA (in days) for those resolved cases per month.
"""

# Count cases resolved per month
df['resolved_month'] = df['resolved_date'].dt.to_period('M')
monthly_sla_count = df.groupby('resolved_month')['case_id'].nunique()

# Average SLA per month
avg_monthly_sla = case_resolution.groupby(case_resolution['resolved_date'].dt.to_period('M'))['sla_days'].mean()

# Combine into a single DataFrame
monthly_sla_df = pd.DataFrame({
    'cases_resolved': monthly_sla_count,
    'avg_sla_days': avg_monthly_sla
}).reset_index()

monthly_sla_df.round(2)

plt.figure(figsize=(12,5))
monthly_sla_count.plot(marker='o', linestyle='-', color='skyblue', title='Number of Cases Resolved per Month')
plt.xlabel('Resolved Month')
plt.ylabel('Number of Cases')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

avg_monthly_sla = case_resolution.groupby(case_resolution['resolved_date'].dt.to_period('M'))['sla_days'].mean()
plt.figure(figsize=(12,5))
avg_monthly_sla.plot(marker='o', linestyle='-', color='orange', title='Average SLA per Month')
plt.xlabel('Month')
plt.ylabel('Average SLA (Days)')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""### SLA vs Issue Type / Activity

Highlights which issue types are slower to resolve
"""

# Merge sla_days from case_resolution back to df
df = df.merge(case_resolution[['sla_days']], left_on='case_id', right_index=True)

sla_by_issue = df.groupby('issue_type')['sla_days'].mean().sort_values(ascending=False)



sla_by_issue.plot(kind='bar', figsize=(10,5), color='lightcoral', edgecolor='black', title='Average SLA by Issue Type')
plt.ylabel('Average SLA (Days)')
plt.show()

# Number of unique cases per issue
unique_cases_per_issue = df.groupby('issue_type')['case_id'].nunique()

# Average SLA per issue
sla_by_issue = df.groupby('issue_type')['sla_days'].mean()

# Combine into a single DataFrame
issue_sla_summary = pd.DataFrame({
    'unique_cases': unique_cases_per_issue,
    'avg_sla_days': sla_by_issue
}).sort_values(by='unique_cases', ascending=False)

issue_sla_summary.round(2)

"""# 5) Deeper-layer Analysis

## Q1: Find Case_ids that share the same start_timestamp at the very beginning, then calculate throughput time, originators, and resources involved in those cases
"""

# 1) Compute the first start timestamp for each case
first_start = df.groupby('case_id')['start_timestamp'].min().reset_index()
first_start.rename(columns={'start_timestamp': 'case_first_start'}, inplace=True)

# 2) Find first_start timestamps shared by multiple cases
duplicate_first_start_ts = first_start['case_first_start'].value_counts()
duplicate_first_start_ts = duplicate_first_start_ts[duplicate_first_start_ts > 1].index

# 3) Get the case_ids of cases with duplicate first start timestamps
duplicate_cases_ids = first_start[first_start['case_first_start'].isin(duplicate_first_start_ts)]['case_id']

# 4) Filter the original dataframe for these cases to get all their events
cases_full = df[df['case_id'].isin(duplicate_cases_ids)]

# 5) Compute case_start and case_end from the original df
throughput = cases_full.groupby('case_id').agg(
    case_start=('start_timestamp', 'min'),
    case_end=('start_timestamp', 'max')
)

throughput['throughput_hours'] = ((throughput['case_end'] - throughput['case_start'])
                                  .dt.total_seconds() / 3600).round(2)

# 6) Extract originators and resources for these cases
originators_resources = cases_full.groupby('case_id').agg(
    originators=('originator', lambda x: sorted(x.dropna().unique().tolist())),
    resources=('resource', lambda x: sorted(x.dropna().unique().tolist()))
)

# 7) Combine and sort by case_start descending
final_result = throughput.join(originators_resources).sort_values('case_start', ascending=False)

final_result

print("Number of cases shared the same start time at the very beginning is:")
print(len(final_result))
print("\n")
print("Number of unique time stamps are: ")
print(len(final_result['case_start'].unique()))
print("\n")
print("Involved Originators are:")
print(final_result['originators'].value_counts())
print("\n")
print("Involved Resources are:")
print(final_result['resources'].value_counts())

"""## Q2: Group cases that shared the same start time, and find cases with the min and max throughput time at that group"""

# Start from final_result (already computed in your last code)
# final_result has: case_start, case_end, throughput_hours, originators, resources

final_result_group = final_result.reset_index()

# 1) Group cases by their case_start timestamp

grouped = final_result_group.groupby('case_start').agg(
    cases_list=('case_id', list),
    max_throughput_case=('throughput_hours', lambda x: final_result_group.loc[x.idxmax(), 'case_id']),
    max_throughput_value=('throughput_hours', 'max'),
    min_throughput_case=('throughput_hours', lambda x: final_result_group.loc[x.idxmin(), 'case_id']),
    min_throughput_value=('throughput_hours', 'min')
)


# 2) Sort by case_start descending (latest first)
grouped = grouped.sort_index(ascending=False)

grouped

"""# 6) Bottleneck Analysis

These analyses cover activity-level, resource-level, originator-level, case-level, rework, activity transition bottlenecks, and month-over-month trends.

First, we need to calculate "waiting time" between activities as we did at 4-D but here we use another name "activity_duration":
"""

# Calculate waiting time between events inside each case
df = df.sort_values(['case_id', 'start_timestamp'])

df['next_timestamp'] = df.groupby('case_id')['start_timestamp'].shift(-1)

df['activity_duration_hours'] = (
    (df['next_timestamp'] - df['start_timestamp'])
    .dt.total_seconds() / 3600
).round(2)

# Given that 1 month = 30 days = 720 hours
df['activity_duration_months'] = (
    (df['next_timestamp'] - df['start_timestamp'])
         .dt.total_seconds() / (3600 * 24 * 30)
).round(2)

"""## 6-1) Activity-Level

We measure how long each activity stays “active” in a case ➡ This identifies slow activities.
"""

# Compute average duration per activity
activity_bottlenecks = df.groupby('activity')[['activity_duration_hours','activity_duration_months']].mean().sort_values(by='activity_duration_hours', ascending=False).round(2)

activity_bottlenecks

activity_bottlenecks['activity_duration_months'].plot(kind='bar', figsize=(12,5), color='tomato')
plt.title("Activity Bottlenecks (Average Duration in Months)")
plt.ylabel("Months")
plt.show()

"""## 6-2) Resource-Level

Which resources slow down the process?
"""

resource_bottlenecks = df.groupby('resource')[['activity_duration_hours','activity_duration_months']].mean().sort_values(by='activity_duration_hours', ascending=False).round(2)
resource_bottlenecks

"""## 6-3) Originator-Level

Which originators cause delays?
"""

originator_bottlenecks = df.groupby('originator')[['activity_duration_hours','activity_duration_months']].mean().sort_values(by='activity_duration_hours', ascending=False).round(2)
originator_bottlenecks

"""## 6-4) Case-Level

Which cases have the longest throughput times?

We already calculated throughput_hours in case_times at section 3.
"""

long_cases_bottleneck = case_times.sort_values(by='case_duration_hours', ascending=False)
long_cases_bottleneck

# Filter the DataFrame to show only rows where case_duration_months is greater than 2 years or 24 months

filtered_long_cases = long_cases_bottleneck[long_cases_bottleneck['case_duration_months'] >= 24]
filtered_long_cases

print("% of cases took less than half a year or 6 month is:")
percentage = (len(long_cases_bottleneck[long_cases_bottleneck['case_duration_months'] <= 6]) / len(long_cases_bottleneck['case_duration_months'])) * 100
print(round(percentage, 2), "%")
print("\n")


print("% of cases took more than half a year and less than a year:")
percentage = (len(long_cases_bottleneck[(long_cases_bottleneck['case_duration_months'] > 6) & (long_cases_bottleneck['case_duration_months'] <= 12)]) / len(long_cases_bottleneck['case_duration_months'])) * 100
print(round(percentage, 2), "%")
print("\n")

print("% of cases took less a year or 12 month is:")
percentage = (len(long_cases_bottleneck[long_cases_bottleneck['case_duration_months'] <= 12]) / len(long_cases_bottleneck['case_duration_months'])) * 100
print(round(percentage, 2), "%")
print("\n")
print("% of cases took more than 1 year or 12 month is:")
percentage = (len(long_cases_bottleneck[long_cases_bottleneck['case_duration_months'] > 12]) / len(long_cases_bottleneck['case_duration_months'])) * 100
print(round(percentage, 2), "%")
print("\n")

print("% of cases took more than 2 year or 24 month is:")
percentage = (len(long_cases_bottleneck[long_cases_bottleneck['case_duration_months'] > 24]) / len(long_cases_bottleneck['case_duration_months'])) * 100
print(round(percentage, 2), "%")
print("\n")

"""## 6-5) Rework Bottlenecks

Cases that repeat the same activity multiple times → rework → bottleneck.
"""

# Step 1 — Count how many times each activity appears per case
rework = df.groupby(['case_id', 'activity']).size().reset_index(name='count')

# Step 2 — Keep only activities with real repetition
rework = rework[rework['count'] > 1]

# Step 3 — Count number of repeated activities per case
rework_cases = rework.groupby('case_id')['activity'].count().rename('rework_activity_count')

# Step 4 — Find the most repeated activity per case
most_repeated_activity = (
    rework.sort_values(['case_id', 'count'], ascending=[True, False])
          .groupby('case_id')
          .first()[['activity', 'count']]
          .rename(columns={'activity': 'most_repeated_activity',
                           'count': 'repeat_count'})
)

# Step 5 — Combine results
rework_summary = pd.concat([rework_cases, most_repeated_activity], axis=1)

rework_summary.sort_values('rework_activity_count', ascending=False)

"""## 6-6) Activity Transition Bottlenecks

Which transitions (A → B) cause delays?
"""

#STEP 1 — Create activity duration per event
# Sort events in correct order
"""
df = df.sort_values(['case_id', 'start_timestamp'])

# Create the next timestamp for duration calculation
df['next_timestamp'] = df.groupby('case_id')['start_timestamp'].shift(-1)

# Calculate duration of each activity in HOURS
df['activity_duration_hours'] = (
    (df['next_timestamp'] - df['start_timestamp'])
        .dt.total_seconds() / 3600
)

"""
#STEP 2 — Create next activity column
# Create next activity
df['next_activity'] = df.groupby('case_id')['activity'].shift(-1)

#STEP 3 — Compute average transition times
# Duration already exists as activity_duration_hours and activity_duration_months
# Calculate average transition times and convert to DataFrame
transition_times_df = df.groupby(['activity', 'next_activity']).agg(
    avg_duration_hours=('activity_duration_hours', 'mean'),
    avg_duration_months=('activity_duration_months', 'mean')
).sort_values(by='avg_duration_hours', ascending=False).reset_index()


transition_times_df.round(2)

transition_times_df.describe().round(2)

"""## 6-7) Month-over-Month Bottleneck Trends

Showing process performance trend.
"""

# Prepare month and year columns
df['year'] = df['start_timestamp'].dt.year
df['month'] = df['start_timestamp'].dt.month

# Aggregate average activity duration per year-month
monthly_slowdowns = (
    df.groupby(['year','month'])['activity_duration_hours']
      .mean()
      .reset_index()
)

# Sequential x-values
x = range(len(monthly_slowdowns))
y = monthly_slowdowns['activity_duration_hours']

plt.figure(figsize=(16,5))
plt.plot(x, y, marker='o')

# --- Create month labels (1,2,3...) ---
month_labels = monthly_slowdowns['month'].astype(str)

# --- Plot month numbers on bottom row ---
plt.xticks(ticks=x, labels=month_labels, rotation=0)

# --- Add year labels with arrows on top row ---
years = monthly_slowdowns['year']
unique_years = years.unique()

for yr in unique_years:
    # Get the positions for this year
    positions = monthly_slowdowns[monthly_slowdowns['year']==yr].index
    start_pos = positions.min()
    end_pos = positions.max()

    # Place year label above the top of the plot, centered on year span
    plt.text(
        x=(start_pos + end_pos)/2,
        y=max(y)*1.05,   # slightly above the max value
        s=f"{yr} →",
        ha='center',
        va='bottom',
        fontsize=10,
        fontweight='bold'
    )

plt.title("Monthly Average Activity Duration — Bottleneck Trend")
plt.ylabel("Hours")
plt.xlabel("Months")
plt.ylim(top=max(y)*1.15)  # add some headroom for year labels
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""# 7) Monthly Summary Table

### The  most involved features in each month

A monthly workload table that not only shows average workload per month but also identifies the most involved resource, originator, issue type, and effort hours for that month.
"""

# Extract month number
df['month'] = df['start_timestamp'].dt.month

# Define aggregation functions
agg_funcs = {
    'activity_duration_hours': 'mean',      # average workload
    'activity_duration_months': 'mean',      # average workload
    'effort_hours': 'sum',                  # total effort in month
    'resource': lambda x: x.value_counts().idxmax(),    # most frequent resource
    'originator': lambda x: x.value_counts().idxmax(),  # most frequent originator
    'issue_type': lambda x: x.value_counts().idxmax(),  # most frequent issue type
    'activity': lambda x: x.value_counts().idxmax()     # most frequent activity
}

# Group by month and aggregate
monthly_summary = df.groupby('month').agg(agg_funcs).reset_index()

# Rename columns
monthly_summary.columns = [
    'Month', 'Avg_waiting_Hours','Avg_waiting_Months', 'Total_Effort_Hours',
    'Most_Active_Resource', 'Most_Active_Originator',
    'Most_Common_Issue_Type', 'Most_Involved_Activity'
]

# Optional: add month names
monthly_summary['Month_Name'] = monthly_summary['Month'].apply(
    lambda x: pd.to_datetime(str(x), format='%m').strftime('%b')
)
# Reorder columns to put Month_Name next to Month
monthly_summary = monthly_summary[
    ['Month', 'Month_Name',
     'Avg_waiting_Hours','Avg_waiting_Months', 'Most_Involved_Activity',
     'Most_Active_Resource', 'Most_Active_Originator',
     'Most_Common_Issue_Type','Total_Effort_Hours']
]

# Sort by average workload descending
monthly_summary = monthly_summary.sort_values('Avg_waiting_Hours', ascending=False)
monthly_summary.round(2)

"""<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4df60853-7a15-4a5a-90b9-fad5d9cfd31d' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>
"""